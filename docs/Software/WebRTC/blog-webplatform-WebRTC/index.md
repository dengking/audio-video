# webplatform [WebRTC](https://webplatform.github.io/docs/concepts/Internet_and_Web/webrtc/)



## My first WebRTC

WebRTC applications need to do several things:

1、Get streaming audio, video or other data.

2、Get network information such as IP address and port, and exchange this with other WebRTC clients (known as *peers*) to enable connection, even through [NATs](http://en.wikipedia.org/wiki/NAT_traversal) and firewalls.

> NOTE:
>
> 实现P2P通信

3、Coordinate signaling communication to report errors and initiate or close sessions. (There is detailed discussion of the network and signaling aspects of WebRTC [below](https://webplatform.github.io/docs/concepts/Internet_and_Web/webrtc/#signaling).)

> NOTE:
>
> 这里的session指的是什么？是否是指在 《**WebRTC音视频实时互动技术：原理、实战与源码分析**》中所说的:
>
> > 实现业务层的管理，如用户创建房间，加入房间，退出房间等。

4、Exchange information about media and client capability, such as resolution and codecs.

5、Communicate streaming audio, video or data.

## MediaStream (aka getUserMedia)

The MediaStream ([specs](https://dvcs.w3.org/hg/audio/raw-file/tip/streams/StreamProcessing.html), [docs](https://webplatform.github.io/docs/apis/webrtc/MediaStream)) represents synchronized streams of media. For example, a stream taken from camera and microphone input has synchronized video and audio **tracks**. (Don’t confuse MediaStream tracks with the `<track>` element, which is something [entirely different](http://www.html5rocks.com/en/tutorials/track/basics/).)

> NOTE:
>
> MediaStream=="a stream taken from camera and microphone input has synchronized video and audio **tracks"**



Each MediaStream has:

1、an input, which might be a `LocalMediaStream` generated by `navigator.getUserMedia()`, 

2、an output, which might be passed to a video element or an RTCPeerConnection.

> NOTE:
>
> 显然，这种设计是非常便于pipeline的。



The `getUserMedia()` method takes three parameters:

- A [constraints object](https://webplatform.github.io/docs/concepts/Internet_and_Web/webrtc/#toc-constraints).
- A success callback which, if called, is passed a `LocalMediaStream`.
- A failure callback which, if called, is passed an error object.



Each `LocalMediaStream` (i.e. a `MediaStream` local to the browser) has a `label` (such as’Xk7EuLhsuHKbnjLWkW4yYGNJJ8ONsgwHBvLQ’) and `audioTracks` and `videoTracks` properties, each of which is a `MediaStreamTrackList`.

For the [simpl.info/gum](http://simpl.info/getusermedia/) example, `audioTracks` and `videoTracks` each have one member, i.e. one `MediaStreamTrack`. Each `MediaStreamTrack` has a kind (‘video’ or ‘audio’), and a label (something like 'FaceTime HD Camera (Built-in)'), and represents one or more channels of either audio or video. In this case, there is only one audio and one video track, but it is easy to imagine use cases where there are more: for example, a chat application that enables input from front camera, rear camera a microphone, and a ‘screenshared’ application.

In Chrome, the `URL.createObjectURL()` method converts a `LocalMediaStream` to a [Blob URL](http://www.html5rocks.com/tutorials/workers/basics/#toc-inlineworkers-bloburis) which can be set as the `src` of a video element. (In Firefox and Opera, the `src` of the video can be set from the stream itself.) Since version 25, Chrome allows audio data from `getUserMedia` to be passed to an audio or video element (but note that by default the media element will be muted in this case.

`getUserMedia` can also be used [as an input node for the Web Audio API](http://updates.html5rocks.com/2012/09/Live-Web-Audio-Input-Enabled):

```javascript
function gotStream(stream) {
  var audioContext = 'webkitAudioContext' in window ?
    new webkitAudioContext() : new AudioContext();
  var mediaStreamSource = audioContext.createMediaStreamSource(stream);
  mediaStreamSource.connect(audioContext.destination);
}

navigator.webkitGetUserMedia({audio:true}, gotStream);
```

Chrome apps and extensions can also incorporate `getUserMedia`. Adding `audioCapture` and/or `videoCapture` [permissions](https://developer.chrome.com/extensions/manifest.html#permissions) to the manifest enables permission to be requested and granted only once, on installation. Thereafter the user is not asked for permission for camera or microphone access. Likewise, on pages using HTTPS, calls to `getUserMedia()` in Chrome will cause an Always Allow button to be displayed in the browser’s [infobar](http://dev.chromium.org/user-experience/infobars): permission only has to be granted once.

Chrome apps also now make it possible to share a live ‘video’ of a browser tab via the experimental [chrome.tabCapture](http://developer.chrome.com/dev/extensions/tabCapture.html) API. For a screencast, code and more information, see the HTML5 Rocks update: [Screensharing with WebRTC](http://updates.html5rocks.com/2012/12/Screensharing-with-WebRTC).

The intention is eventually to enable a MediaStream for any streaming data source, not just a camera or microphone, which would enable collection of arbitrary real-time data, for example from sensors or other inputs.

Note that `getUserMedia()` must be used on a server, not the local file system, otherwise a `PERMISSION_DENIED: 1` error will be thrown.





## Signaling: session control, network and media information

WebRTC uses `RTCPeerConnection` to communicate streaming data between browsers (aka peers), but also needs a mechanism to coordinate communication and to send control messages, a process known as **signaling**. Signaling methods and protocols are *not* specified by WebRTC: signaling is not part of the RTCPeerConnection API.